---
subtitle: "TMA4268 Statistical Learning V2023"
title: "Compulsory exercise 1: Group 11"
author: "Torbjørn Vatne, Ludvik Braathen and Johan Bjerkem"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document
  # pdf_document
---
  
```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",fig.width=4, fig.height=3,fig.align = "center")
```

```{r,eval=FALSE,echo=TRUE}
install.packages("knitr")     # probably already installed
install.packages("rmarkdown") # probably already installed
install.packages("ggplot2")   # plotting with ggplot2
install.packages("dplyr")     # for data cleaning and preparation
install.packages("tidyr")     # also data preparation
install.packages("carData")   # dataset
install.packages("class")     # for KNN
install.packages("pROC")      # calculate roc
install.packages("plotROC")   # plot roc
install.packages("ggmosaic")  # mosaic plot
install.packages("knitr")
install.packages("formatR")
library(knitr)
install.packages("tinytex")
tinytex::install_tinytex()
```

<!--  Etc (load all packages needed). -->


# Problem 1 (9P)

 We consider the following regression problem

$$
Y=f(\mathbf {x})+\varepsilon, \text{ where } \text{E}(\varepsilon)=0 \text{ and } \text{Var}(\varepsilon)=\sigma^2.
$$

Assume now that the true function $f(\mathbf {x})$ is a linear combination of the observed covariates, that is
$f(\mathbf{x}) = \mathbf{x}^T\boldsymbol{\beta}=\beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p,$ where $\mathbf{x}$ and $\boldsymbol{\beta}$ are both vectors of length $p+1.$

We know that the OLS estimator in this case is $\hat{\boldsymbol{\beta}} = (\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{Y}$, with design matrix $\mathbf{X}$ and response vector $\mathbf{Y}$. In this task we will look at a competing estimator $\widetilde{\boldsymbol \beta} =(\mathbf{X}^\top\mathbf{X}+\lambda {\bf I})^{-1}\mathbf{X}^\top{\bf Y}$ (this is called the ridge regression estimator), where $\lambda\ge0$ is  a constant tuning parameter that controls the bias-variance trade-off. Observe that for $\lambda = 0$ the ridge regression estimator is equivalent to $\hat{\boldsymbol{\beta}}.$

We will first derive mathematical formulas for the bias and variance of $\widetilde{\boldsymbol \beta}$ and then we will plot these curves in R.

## a) (1P)
Find the expected value and the variance-covariance matrix of $\widetilde{\boldsymbol \beta}.$

Here we find the expected value:

$$
\displaylines{
    E(\widetilde{\boldsymbol \beta})=E((\mathbf{X}^\top\mathbf{X}+\lambda {\bf I})^{-1}\mathbf{X}^\top{\bf Y})\cr
    E(\widetilde{\boldsymbol \beta})=(\mathbf{X}^\top\mathbf{X}+\lambda {\bf I})^{-1}\mathbf{X}^\top{E(\bf Y})\cr
    E(\widetilde{\boldsymbol \beta})=(\mathbf{X}^\top\mathbf{X}+\lambda {\bf I})^{-1}\mathbf{X}^\top{E(\bf X \boldsymbol \beta + \varepsilon})\cr
    E(\widetilde{\boldsymbol \beta})=(\mathbf{X}^\top\mathbf{X}+\lambda {\bf I})^{-1}\mathbf{X}^\top{(\bf X \boldsymbol \beta + 0})\cr
    E(\widetilde{\boldsymbol \beta})=(\mathbf{X}^\top\mathbf{X}+\lambda {\bf I})^{-1}\mathbf{X}^\top{\bf X \boldsymbol \beta}\cr
}
$$
Here we find the variance-covarince matrix:

\begin{align}
Cov(\widetilde{\boldsymbol \beta})&=Cov((X^T X + \lambda I)^{-1}X^T Y)\\
&=(X^T X + \lambda I)^{-1}X^T Cov(Y)((X^TX + \lambda I)^{-1}X^T)^T \\
&=(X^TX + \lambda I)^{-1}X^T \sigma^2  I ((X^TX + \lambda I)^{-1}X^T)^T
\end{align}

## b) (2P)
Let $\widetilde{f}(\mathbf{x}_0)=\mathbf{x}_0^T \widetilde{\boldsymbol{\beta}}$ be the prediction at a new covariate vector $\mathbf{x}_0.$ 
Using a), find the expected value and variance for $\widetilde{f}(\mathbf{x}_0)=\mathbf{x}_0^T \widetilde{\boldsymbol{\beta}}.$

First we find the expecrted value:

$$
\displaylines{
    E(\mathbf{x}_0^T \widetilde{\boldsymbol{\beta}})=\mathbf{x}_0^T E(\widetilde{\boldsymbol{\beta}})\cr
    E(\mathbf{x}_0^T \widetilde{\boldsymbol{\beta}})=\mathbf{x}_0^T (\mathbf{X}^\top\mathbf{X}+\lambda {\bf I})^{-1}\mathbf{X}^\top{\bf X \boldsymbol \beta}\cr
}
$$
Then we find the variance:
(Here we need to remember the rule for variance where we need to multiply the same term transposed at the back of the variance term.)

$$
\displaylines{
    Var(\mathbf{x}_0^T \widetilde{\boldsymbol{\beta}})=\mathbf{x}_0^T Var(\widetilde{\boldsymbol{\beta}}) \mathbf{x}_0\cr
    Var(\mathbf{x}_0^T \widetilde{\boldsymbol{\beta}})=\mathbf{x}_0^T Var(\widetilde{\boldsymbol{\beta}}) \mathbf{x}_0\cr
     Var(\mathbf{x}_0^T \widetilde{\boldsymbol{\beta}})=\mathbf{x}_0^T (\mathbf{X}^\top\mathbf{X}+\lambda {\bf I})^{-1}\mathbf{X}^\top{\sigma^2}((\mathbf{X}^\top\mathbf{X}+\lambda {\bf I})^{-1}\mathbf{X}^\top)^\top \mathbf{x}_0
}
$$

## c) (1P)
Explain with words how we can interpret the three terms: bias, variance and irreducible error.

Bias is the error that comes by oversimplifying the model. This means underfitting the model, meaning high bias will over simplify the relationships in the data.
Variance, on the other hand, is the error that comes by creating a too complex mode. This means overfitting the model, where the model tries to hard to fit the function to the noise of the data. There is a famous plot of the relationship between bias and variance. When we train a model we start with a high bias that decays as we train the model. The variance starts low and increases as we train the model. We want to hit the sweet spot where bias and variance intercepts, as this is the point with the minimal total error.
Irreducible error is the error that comes with the data. As the name suggests this error cannot be eliminated by any model, as it is inherit by the data and is not due to bias or variance.


## d) (2P)
Find the expected MSE at $\mathbf{x}_0,$ $E[(y_0 - \widetilde{f}(\mathbf{x}_0))^2]$. 

*Hint*: Use that the expected MSE can be decomposed into squared bias, variance and irreducible error, and then move on from there.

$$
\displaylines{
    E[(y_0 - \widetilde{f}(\mathbf{x}_0))^2]=Var(\widetilde{f}(\mathbf{x}_0)) + (Bias(\widetilde{f}(\mathbf{x}_0)))^2 + Var(\varepsilon)\cr\cr
    
    Var(\widetilde{f}(\mathbf{x}_0)) = \mathbf{x}_0^T (\mathbf{X}^\top\mathbf{X}+\lambda {\bf I})^{-1}\mathbf{X}^\top{\sigma^2}((\mathbf{X}^\top\mathbf{X}+\lambda {\bf I})^{-1}\mathbf{X}^\top)^\top \mathbf{x}_0\cr\cr
    
    (Bias(\widetilde{f}(\mathbf{x}_0)))^2 = (E(\mathbf{y}_0 - \widetilde{f}(\mathbf{x}_0)))^2\cr\
    (Bias(\widetilde{f}(\mathbf{x}_0)))^2 = (E(\mathbf{y}_0) - E(\widetilde{f}(\mathbf{x}_0)))^2\cr
        (Bias(\widetilde{f}(\mathbf{x}_0)))^2 = (\mathbf{y}_0 - \mathbf{x}_0^T (\mathbf{X}^\top\mathbf{X}+\lambda {\bf I})^{-1}\mathbf{X}^\top{\bf X \boldsymbol \beta})^2\cr\cr

    Var(\varepsilon) = {\sigma^2}\cr\cr
    E[(y_0 - \widetilde{f}(\mathbf{x}_0))^2]=\mathbf{x}_0^T (\mathbf{X}^\top\mathbf{X}+\lambda {\bf I})^{-1}\mathbf{X}^\top{\sigma^2}((\mathbf{X}^\top\mathbf{X}+\lambda {\bf I})^{-1}\mathbf{X}^\top)^\top \mathbf{x}_0 + (Bias(\widetilde{f}(\mathbf{x}_0)))^2 + {\sigma^2}\cr
}
$$

### Plotting the bias-variance trade-off

The estimator $\widetilde{\boldsymbol\beta}$ is a function of the tuning parameter $\lambda$, which controls the bias-variance trade-off. Using the decomposition derived in c) we will plot the three elements (bias, variance and irreducible error) using the values in the code chunk below. `values` is a list with the design matrix `X`, the vector $\mathbf{x}_0$ as `x0`, the parameters vector `beta` and the irreducible error `sigma`.

```{r}
id <- "1X_8OKcoYbng1XvYFDirxjEWr7LtpNr1m" # google file ID
values <- dget(sprintf("https://docs.google.com/uc?id=%s&export=download", id))
X <- values$X
dim(X)
x0 <- values$x0
dim(x0)
beta <- values$beta
dim(beta)
sigma <- values$sigma
sigma
```


## e) (1P)

First we will create the squared bias function (`bias`) which takes as inputs the parameter `lambda`, `X`, `x0`, `beta` and returns the squared bias. You have only to fill the `value <- ...` and run the chunk of code to plot the squared bias, where value is the squared bias as derived in d). 

```{r,eval=FALSE}
library(ggplot2)
bias <- function(lambda, X, x0, beta) {
  p <- ncol(X)
  inv <- solve(t(X) %*% X + lambda * diag(p))
  value <- (t(x0)%*%(t(X)%*%X-lambda*diag(p))^-1%*%t(X)%*%X%*%beta)^2
  return(value)
}
lambdas <- seq(0, 2, length.out = 500)
BIAS <- rep(NA, length(lambdas))
for (i in seq_along(lambdas)) BIAS[i] <- bias(lambdas[i], X, x0, beta)
dfBias <- data.frame(lambdas = lambdas, bias = BIAS)
ggplot(dfBias, aes(x = lambdas, y = bias)) +
  geom_line(color = "hotpink") +
  xlab(expression(lambda)) +
  ylab(expression(bias^2))
```

## f) (1P)

Now we will create the variance function which takes the same inputs as the squared bias. As in e) you have to fill only the `value <- ... ` and run the code to plot the variance.

```{r,eval=FALSE}
variance <- function(lambda, X, x0, sigma) {
  p <- ncol(X)
  inv <- solve(t(X) %*% X + lambda * diag(p))
  print(inv)
  value <- ...
  return(value)
}
lambdas <- seq(0, 2, length.out = 500)
VAR <- rep(NA, length(lambdas))
for (i in seq_along(lambdas)) VAR[i] <- variance(lambdas[i], X, x0, sigma)
dfVar <- data.frame(lambdas = lambdas, var = VAR)
ggplot(dfVar, aes(x = lambdas, y = var)) +
  geom_line(color = "gold") +
  xlab(expression(lambda)) +
  ylab("variance")
```



## g) (1P)
Fill in the `exp_mse` of the following code to calculate the expected MSE for the same lambda values that you plugged in above. Then plot all the components together and find the value of $\lambda$ which minimizes the expected MSE. 

```{r,eval=FALSE}
exp_mse <- ...
lambdas[which.min(exp_mse)]
```


# Problem 2 (15P) 

```{r desc, fig.width=10, fig.height=10, fig.cap="Pairs plot of the academic salary data set.",out.width = '70%'}
library(carData)
GGally::ggpairs(Salaries)
```


```{r load_data}
# Fit full model
?Salaries
model1 <- lm(salary ~ rank + discipline + yrs.since.phd + yrs.service + sex, data = Salaries)
summary(model1)
plot(model1)
```

## a) Categorical variables (2P)

i) The $lm()$ function by default uses the first of the three levels (of the covariate rank), `AsstProf`, as a reference level, and then converts the other levels, `AssocProf` and `Prof`, to a set of 'dummy variables'. These dummy variables are represented as binary variables. Naturally, only one of the levels can be 'active' at once, as a professor can only be either of the three different types of professors.
The estimated coefficient for associate professor represents the difference in mean salary between associate professors and assistant professors (reference level), while the estimated coefficient for a full professor represents the difference in mean salary between full professors and assistant professors. In other words, associate professors are expected to earn at average 12907 more than an assistant professor, and full professors are expected to earn at average 45066 more than an assistant professor. ***FIKS FORMATERING***

ii)

```{r rank_test}
library(carData)
r.salary.rank <- lm(salary ~ yrs.service + yrs.since.phd + rank , data = Salaries)
r.salary.no.rank <- lm(salary ~ yrs.service + yrs.since.phd, data = Salaries)
anova(r.salary.rank, r.salary.no.rank)
```
Here we've done a comparison of two models, Model 1 and Model 2. In Model 1 we've included the variables: `yrs.service`, `yrs.since.PhD`, and `rank`. In Model 2 we also include `yrs.service` and `yrs.since.PhD`, but excluded the `rank`-variable. Running $anova()$ on these two models, gives us an $F$-value of 68.648 and a $p$-value of 2.2e-16. Determining which values of an F-test that are considered 'good' is not easy. In this case, with a relatively high $F$-value, together with a $p$-value which is practically 0, we consider that there is enough evidence that `rank` as a whole does have an impact on `salary`.

## b) Conflicting model results? (2P)

```{r sex_model_with_rank}
library(ggplot2)
sex_model <- lm(salary ~ sex, data = Salaries)
summary(sex_model)

ggplot(Salaries, aes(x = sex, y = salary, col = rank)) +
  geom_jitter() +
  labs(x = "Sex", y = "Salary", color = "Rank") +
  scale_x_discrete(labels = c("Female", "Male"))

library(gridExtra)
p1 = ggplot(data = Salaries[Salaries$sex == 'Female',], aes(x = rank, fill = rank)) +
  geom_bar(position = "identity", alpha = 0.5, aes(y = ..prop.., group = 1)) +
  ggtitle("Density of Ranks for Females") +
  ylab("Percentage") +
  scale_y_continuous(labels = scales::percent)

p2 = ggplot(data = Salaries[Salaries$sex == 'Male',], aes(x = rank, fill = rank)) +
  geom_bar(position = "identity", alpha = 0.5, aes(y = ..prop.., group = 1)) +
  ggtitle("Density of Ranks for Males") +
  ylab("Percentage") +
  scale_y_continuous(labels = scales::percent)

grid.arrange(p1, p2, nrow = 2)
```

```{r sex_model_without_rank}
sex_model <- lm(salary ~ sex, data = Salaries)
boxplot(salary~sex, data=Salaries)
```

There can be two reasons:

1. There are relatively few observations on females compared to men, which makes the error variance higher, which then increases the p-value of the predictor. Had there been more female observations, it is likely that Std. error would decrease, which then would make the p-value more significant. With a sexMale parameter of 4783.5 it seems that sex correlates with salary. This is even more clear in the model with sex as the only predictor, where the p-value also is more significant. 

2. Another possible reason why sex seems insignificant in model1 might be that there is another variable explaining the difference in salaries for the two sexes; Rank. There is a high correlation between Rank and salary, and looking at the barplots, we see that a high percentage of men are professors, however a much lower percentage of females are professors. 


## c) Model assumptions and transformation (2P)

```{r fig_model_check, fig.width=6, fig.height=6, fig.cap="Diagnostic plots for `model1`.",out.width = '60%'}
library(ggplot2)
library(ggfortify)
autoplot(model1, smooth.colour = NA)

```

i) In the Residuals vs fitted plot, we can clearly see that the expected value of the residuals are zero, which fulfills the first assumption. However, the variance of the residuals is not constant over the fitted values, meaning there is no homoscedasticity. In other words, the residuals are not normally distributed with a constant variance. This can be seen in the Scale-Location and Normal Q-Q plots too.

The assumption that there are no influentual outliers is unfulfilled as there are high-residual points with high leverage

ii)

```{r fig_model_check_log-transform, fig.width=6, fig.height=6, fig.cap="Diagnostic plots for `model2`.",out.width = '60%'}
log_salary <- log(Salaries$salary)
#Får feilmelding under knitting (pga log_salary elns) dersom jeg skriver det slik:
#model2 <- lm(log_salary ~ . -salary)
model2 <- lm(log_salary ~ rank + discipline + yrs.since.phd + yrs.service + sex - salary, data = Salaries)
summary(model2)
library(ggplot2)
autoplot(model2, smooth.colour = NA)
```

The residual variance has become somewhat more constant, but not much better. Influentual outliers does not seem to have improved.

## d) Interactions (2P)

i)
```{r fig_model_check_interaction, fig.width=6, fig.height=6, fig.cap="Diagnostic plots for `model3`.",out.width = '60%'}
model3 <- lm(log_salary ~ rank + discipline + yrs.since.phd + yrs.service + sex + sex:yrs.since.phd -salary, data = Salaries)
summary(model3)
```

ii) From the summary of the linear model, we can see that the `p`-value for the interaction term is very high, and we conclude with Bert-Ernie's hypothesis being incorrect.

## e) Bootstrap (4P)

(i)
```{r bootstrap}
set.seed(4268)

#Function to extract the R^2-value from the linear model
getR2 <- function(data, indices) {
  fit <- lm(salary ~ rank + discipline + yrs.since.phd + yrs.service + sex, data = data[indices,])
  summary(fit)$r.squared
}

library(boot)
boot_results <- boot(data = Salaries, statistic = getR2, R = 1000, strata = Salaries$rank)
boot_results

```
(ii)

```{r bootstrap_plot}
# Plot the distribution of R-squared values
hist(boot_results$t, main = "Bootstrap Distribution of R-Squared", 
     xlab = "R-Squared", col = "lightblue", border = "white")
abline(v = mean(boot_results$t), col = "red")
```

(iii)

```{r bootstrap_quantiles}

se = sd(boot_results$t)
se

quant = boot_results$t[25:975] # 95%  quantile, removing bottom and top 2.5%.
summary(quant)

```

(iv) Original R-squared was calculated to 0.5249 and bootstrap estimates 0.4546766 with a 95% confidence interval of [0.3761, 0.5470]. From this we see that the original R-squared calculated lies within the 95% confidence interval.


## f) Picking a field (3P)
i)

```{r prediction}
# Make a data frame containing two new observations, corresponding to
# Bert-Ernie's two possible futures
bert_ernie <- data.frame(rank = c("Prof", "Prof"),
                         discipline = c("A", "B"), # Theoretical, applied
                         yrs.since.phd = c(20, 20),
                         yrs.service = c(20, 20),
                         sex = c("Male", "Male"))
# Use the full model to predict his salary
preds <- predict(object = model1,
                 newdata = bert_ernie,
                 interval = "prediction", # corrected confidence to prediction
                 level = 0.95) # Not 0.95 since we don't care about upper limit # Corrected 0.975 to 0.95
# Check predictions
preds
# Check if lower limit for salary in a theoretical field is large enough
preds[1, 2] > 75000
```

Many sleepless nights of debugging R code are looming in his future unfortunately.

ii) The analytic expression for the lower limit of the prediction interval:

$$
\displaylines{
    PI_{lower} = \boldsymbol{x}_0^T \hat{\boldsymbol{\beta}} - t_{n-p} (1-\alpha/2) \hat{\sigma} \sqrt{1+ \boldsymbol{x}_0^T (X^TX)^{-1} \boldsymbol{x}_0}
    
    
  \cr
  \hat{\boldsymbol{\beta}} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y}
}
$$

```{r PI_lower_calculated}
x_0 = c(1, 0, 1, 0, 20, 20, 1)
beta_hat = coef(model1)
alpha = 0.05
sd_hat = summary(model1)$sigma
X <- model.matrix(~ rank + discipline + yrs.since.phd + yrs.service + sex, data = Salaries)

n = nrow(Salaries)


p = ncol(Salaries)-1 # should this be ncol of X or Salaries (-1) ??

qt(1-alpha/2, df=n-p) # this gives the wrong number..
PI_lower = t(x_0) %*% beta_hat - qt(1-alpha/2, df=n-p) * sd_hat * sqrt(1 + t(x_0) %*% solve(t(X) %*% X) %*% x_0)
PI_lower
```

Not correct, don't know what's wrong.

# Problem 3 (13P)
The Bigfoot Field Researchers Organization (BFRO) is an organization dedicated to investigating the bigfoot mystery, and for years they have been collecting reported sightings in a database. They manually classify their reports into

 - Class A: Clear sightings in circumstances where misinterpretation or misidentification of other animals can be ruled out with greater confidence
 - Class B: Incidents where a possible bigfoot was observed at a great distance or in poor lighting conditions and incidents in any other circumstance that did not afford a clear view of the subject.

However, they wonder if this can be automated and done by a classification algorithm instead. So in this task, you will set up a few different classification algorithms for this aim, and evaluate their performance.

Feel free to look at the original data, `bigfoot_original` below, however in this task we will be using a slightly simplified version of the data set where we have extracted some variables of interest and deleted observations that are missing any of these variables of interest.

Download the data as below, and run through the provided cleaning/preparation steps.

```{r,  eval=TRUE}
bigfoot_original <- readr::read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-09-13/bigfoot.csv")
library(dplyr)
# Prepare the data:
bigfoot <- bigfoot_original %>%
  # Select the relevant covariates:
  dplyr::select(classification, observed, longitude, latitude, visibility) %>%
  # Remove observations of class C (these are second- or third hand accounts):
  dplyr::filter(classification != "Class C") %>%
  # Turn into 0/1, 1 = Class A, 0 = Class B:
  dplyr::mutate(class = ifelse(classification == "Class A", 1, 0)) %>%
  # Create new indicator variables for some words from the description:
  dplyr::mutate(fur = grepl("fur", observed),
                howl = grepl("howl", observed),
                saw = grepl("saw", observed),
                heard = grepl("heard", observed)) %>%
  # Remove unnecessary variables:
  dplyr::select(-c("classification", "observed")) %>%
  # Remove any rows that contain missing values:
  tidyr::drop_na()
```

The data we will use for this task includes the following variables:

- `class`: the assigned class of the observation, coded as 1 = Class A, 0 = Class B
- `longitude`: longitude of the observation
- `latitude`: latitude of the observation
- `visibility`: estimated visibility at the time and place of observation (higher value means better visibility)
- `fur`: does the report contain the word "fur"? `(TRUE/FALSE)`
- `howl`: does the report contain the word "howl"? `(TRUE/FALSE)`
- `saw`: does the report contain the word "saw"? `(TRUE/FALSE)`
- `heard`: does the report contain the word "heard"? `(TRUE/FALSE)`

For the following tasks, we will be using all of these variables.

The data is split into test and training sets as follows:

*Please remember to use the same seed when you split the data into training and test set.*

```{r}
set.seed(2023)
# 70% of the sample size for training set
training_set_size <- floor(0.7 * nrow(bigfoot))
train_ind <- sample(seq_len(nrow(bigfoot)), size = training_set_size)
train <- bigfoot[train_ind, ]
test <- bigfoot[-train_ind, ]
```


## a) (2P)

(i) (1P) Fit a **logistic regression** model using the training set, and perform the classification on the test set, using a 0.5 cutoff. How many of the reports were classified as clear sightings (class A, category 1)?

(ii) (1P) Single choice:
According to this model, how would the odds that an observation is classified as Class A change if the report contains the word "saw", compared to if it does not? (all other covariates stay the same)

```{r, echo = FALSE}
beta_saw <- 1.292176
```

  1) We multiply by `r round(beta_saw, 3)`.
  2) We multiply it with `r -round(exp(beta_saw), 3)`.
  3) We multiply by `r round(exp(-beta_saw), 3)`.
  4) We multiply by `r round(exp(beta_saw), 3)`.
  5) We add `r round(exp(beta_saw), 3)`.
  6) We add `r round(beta_saw, 3)`.


## b) (3P)

(i) (1P) Fit a **QDA** model using the training set, and perform the classification on the test set, using a 0.5 cutoff. How many of the reports were classified as class A?

(ii) (2P) Which statements about linear discriminant analysis and quadratic discriminant analysis are true and which are false? Say for *each* of them if it is true or false.

   1) QDA differs from LDA in that it assumes that observations of different classes come from Gaussian distributions with different covariance matrices.
   2) LDA is a better choice in cases where there are a lot of observations and so reducing bias is important.
   3) Even if the Bayes decision boundary is linear, QDA will be a better choice.
   4) QDA assumes that the observations are drawn from a multivariate Gaussian distribution with common mean vector and class-specific covariance matrix


## c) (2P) 
(i) (1P) Fit a **KNN** model using the training set, and perform the classification on the test set, with $k = 25$ (use the `knn` function from the `class` package). 

**R-hints:**
In the `knn()` function set `prob=T` to ensure you get the class probabilities that you then need in d) (change the "`...`" to the correct input):
```{r,eval=FALSE}
knnMod <- knn(train = ..., test = ..., cl = ..., k = 25, prob = TRUE)
```

(ii) (1P) Explain how you could choose the tuning parameter $k$ in a better way. How does $k$ relate to the bias-variance trade-off in this context?

## d) (6P) 
We now wish to compare the performance of the three models (logistic regression, QDA and KNN) on this dataset, in order to report back to BFRO on our results.

i) (2P) In this case, are we interested in prediction or inference? What implications would it have for the model choice if we wanted to do prediction, and what implications would it have if we wanted to do inference? In conclusion, does the question of whether our aim is prediction or inference exclude any of the three candidate models in this case? (comment briefly)

ii) (3P) Make confusion matrices for the three predictions performed on the test set in a) - c), and report the confusion matrices, along with sensitivity and specificity. Explain briefly: What does sensitivity and specificity mean? Feel free to explain using the bigfoot example. 
*Make sure it is clear in the confusion matrix which numbers show the predictions and which show the true values.*

iii) (1P) Present a plot of the ROC curves and calculate the area under the curve (AUC) for each of the classifiers.

iv) (1P) Summarize the performance of the three classifiers with words, based on the evaluations above. Which one would you choose? Justify briefly.

**R-hints:**

* To obtain $P(y=1)$ from the `knn()` output you have to be aware that the respective probabilities `attributes(knnMod)$prob` are the success probability for the actual class where the categorization was made. So if you want to get a vector for $P(y=1)$, you have to use $1-P(y=0)$ for the cases where the categorization was 0:
```{r,eval=FALSE}
probKNN <- ifelse(knnMod == 0,
                  1 - attributes(knnMod)$prob,
                  attributes(knnMod)$prob)
```
* You might find the functions `roc()` and `ggroc()` from the package `pROC` useful, but there are many ways to plot ROC curves.



# Problem 4 (6P)

## a) (4P)
General formula for CV:

$$
CV_{K} = \sum_{i=1}^{n} \frac{n_k}{{N}}MSE_k \ , \ \ \ \  \ \ \ \ \ \  MSE_k = \sum_{i \in C_k} (y_i - \hat{y}_{(-i)})^2
$$
In LOOCV we have $K=N$, which means that $C_k=k$, and $n_k=1$. This gives

$$
CV = \frac{1}{N} \sum_{i=1}^{n} (y_i - \hat{y}_{(-i)})^2
$$
We also have 

$$
\hat{y}_{(-i)} = \boldsymbol{x}_i^\top \hat{\boldsymbol{\beta}}_{(-i)}
\\ \hat{\boldsymbol{\beta}}_{(-i)} = (\mathbf{X}_{(-i)}^\top \mathbf{X}_{(-i)})^{-1} \mathbf{X}_{(-i)}^\top \mathbf{y}_{(-i)} 
$$
Hint 2 and 3 then gives us
$$
\hat{\boldsymbol{\beta}}_{(-i)} = (\mathbf{X}^\top \mathbf{X} - \mathbf{x}_i\mathbf{x}_i^\top)^{-1} (\mathbf{X}^\top \mathbf{y}-\mathbf{x}_i y_i)
$$
The Sherman-Morrison formula then gives

$$
\hat{\boldsymbol{\beta}}_{(-i)} = 
\left(
(\mathbf{X}^\top \mathbf{X})^{-1}
- \frac{
(\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{x}_i\mathbf{x}_i^\top (\mathbf{X}^\top \mathbf{X})^{-1}
}
{
1+ \mathbf{x}_i^\top (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{x}_i
}
\right)
(\mathbf{X}^\top \mathbf{y}-\mathbf{x}_i y_i )
\\
\Rightarrow \hat{y}_{(-i)} = 
\boldsymbol{x}_i^\top
\left(
(\mathbf{X}^\top \mathbf{X})^{-1}
- \frac{
(\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{x}_i\mathbf{x}_i^\top (\mathbf{X}^\top \mathbf{X})^{-1}
}
{
1+ h_i
}
\right)
(\mathbf{X}^\top \mathbf{y}-\mathbf{x}_i y_i )
\\
\Rightarrow 
y_i - \hat{y}_{(-i)} = 
y_i - 
\boldsymbol{x}_i^\top
\left(
(\mathbf{X}^\top \mathbf{X})^{-1}
- \frac{
(\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{x}_i\mathbf{x}_i^\top (\mathbf{X}^\top \mathbf{X})^{-1}
}
{
1+ h_i
}
\right)
(\mathbf{X}^\top \mathbf{y}-\mathbf{x}_i y_i )
=^? \frac{y_i-\hat{y}_i}{1-h_i}
$$


## b) Multiple choice (2P)
FALSE, FALSE, TRUE, FALSE